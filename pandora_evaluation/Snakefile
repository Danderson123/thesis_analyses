# Snakefile
import os
import glob
import pandas as pd
import pyfastaq
import random
import statistics
from tqdm import tqdm
import json

# Define the input directory with FASTA files
ASSEMBLY_DIR = "../pandora_assemblies"
# get the samples
SAMPLES = [os.path.basename(f).replace(".fasta", "").replace(".fna", "").replace(".fa", "") for f in glob.glob(os.path.join(ASSEMBLY_DIR, "*"))]
# Define the output directory
OUT_DIR = "pandora_assessment_with_AMR"
# Define the clustering parameters
identities = [0.80, 0.85, 0.90, 0.95]
lengths = [0, 0.50, 0.90]
# Define the Pandora k-mer and window size
kmer_sizes = [11, 15, 19, 23, 27]
window_sizes = [5, 10, 14, 18, 22, 26]
# make the output directory if it does not exist
if not os.path.exists(OUT_DIR):
    os.mkdir(OUT_DIR)

def get_valid_outputs(initial_list):
    valid_outputs = []
    for f in initial_list:
        w = int(f.split(".w")[1].split(".")[0])
        k = int(f.split(".w")[0].split(".k")[1])
        if k >= w:
            valid_outputs.append(f)
    return valid_outputs

# Add a rule all to specify final outputs
rule all:
    input:
        "result_plots_k_vs_w.eval.pdf",
        "result_plots_identity_vs_length.eval.pdf",
        expand(
                os.path.join(OUT_DIR, "0.8_0", "summary_stats.{sample}.k15.w5.txt"),
                sample=SAMPLES
                )

checkpoint get_assemblies:
    input:
        assembly_dir=ASSEMBLY_DIR
    output:
        directory(os.path.join(OUT_DIR, "subsampled_assemblies"))
    run:
        # make the output directory
        if not os.path.exists(output[0]):
            os.mkdir(output[0])
        input_files=glob.glob(os.path.join(input.assembly_dir, "*"))
        for f in input_files:
            shell(f"cp {f} {output[0]}")
            if ".gz" in f and not os.path.exists(f.replace(".gz", "")):
                to_decompress = os.path.join(output[0], os.path.basename(f))
                shell(f"gunzip {to_decompress}")
            if ".fna" in f:
                name = os.path.join(output[0], os.path.basename(f).replace(".gz", "").replace(".fna", ".fa"))
                shell(f"mv {os.path.join(output[0], os.path.basename(f)).replace('.gz', '')} {name}")

rule run_bakta:
    input:
        os.path.join(OUT_DIR, "subsampled_assemblies", "{sample}.fa")
    output:
        directory(os.path.join(OUT_DIR, "bakta_annotated_assemblies", '{sample}'))
    params:
        bakta_db="../bakta_db"
    conda: "envs/bakta.yaml"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: 20000 * attempt, threads=1
    run:
        "bakta --skip-trna --skip-tmrna --skip-rrna --skip-ncrna --skip-crispr --skip-plot --skip-sorf --skip-pseudo --skip-ncrna-region  --db {params.bakta_db} --threads {threads} --output {output} {input}"

def get_samples():
    # This function should return a list of all samples that are processed by 'run_bakta'
    return [os.path.basename(f).replace(".fa", "") for f in glob.glob(os.path.join(checkpoint_output, "*.fa*"))]

def get_bakta_files(wildcards):
    checkpoint_output = checkpoints.get_assemblies.get(**wildcards).output[0]
    return expand(os.path.join(OUT_DIR, "bakta_annotated_assemblies", "{sample}"), sample=[os.path.basename(f).replace(".fa", "") for f in glob.glob(os.path.join(checkpoint_output, "*.fa*"))])

rule list_bakta_gffs:
    input:
        get_bakta_files
    output:
        os.path.join(OUT_DIR, "panaroo_input.txt")
    run:
        # list all gff files output by bakta
        gff_files = glob.glob(os.path.join(os.path.dirname(input[0]), "*", "*.gff*")) + ["AMR_alleles.gff"]
        with open(output[0], 'w') as file_out:
            file_out.write("\n".join(gff_files))

rule run_panaroo:
    input:
        rules.list_bakta_gffs.output
    output:
        directory(os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_output"))
    params:
        length_outlier_support_proportion=0.1,
        i="{identity}",
        l="{length}"
    threads: 24
    conda: "envs/panaroo.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: 40000 * attempt, threads=24
    shell:
        """
        mkdir -p {output} &&
        panaroo --clean-mode sensitive -c {params.i} --len_dif_percent {params.l} \
            --length_outlier_support_proportion {params.length_outlier_support_proportion} \
            --refind-mode off --remove-invalid-genes --merge_paralogs \
            -i {input} -o {output} --threads {threads}
        """

rule get_panaroo_alignments:
    input:
        rules.run_panaroo.output
    output:
        touch(os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_alignments.done"))
    threads: 24
    conda: "envs/panaroo.yaml"
    resources:
	    mem_mb=lambda wildcards, attempt: 40000 * attempt, threads=24
    shell:
        'panaroo-msa -o {input} -a pan --aligner mafft -t {threads}'

rule rename_alignments:
    input:
        rules.get_panaroo_alignments.output,
        panaroo_dir=os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_output")
    output:
        touch(os.path.join(OUT_DIR, "{identity}_{length}", "rename_alignments.done"))
    threads: 1
    resources:
	    mem_mb=lambda wildcards, attempt: 40000 * attempt, threads=1
    run:
        fastas = glob.glob(os.path.join(os.path.join(input.panaroo_dir, "aligned_gene_sequences"), "*"))
        for f in fastas:
            if ".aln.fas" in f:
                os.rename(f, f.replace(".aln.fas", ".fasta").replace("~~~", "."))

rule make_prg:
    input:
        os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_output"),
        rules.rename_alignments.output
    output:
        os.path.join(OUT_DIR, "{identity}_{length}", "panRG.prg.fa")
    threads: 16
    resources:
        mem_mb=lambda wildcards, attempt: 40000 * attempt, threads=16
    run:
        prefix = output[0].replace(".prg.fa", "")
        shell("make_prg from_msa -F -i {input[0]}/aligned_gene_sequences --output-prefix {prefix} --threads {threads}")

rule build_pandora_index:
    input:
        prg=rules.make_prg.output
    output:
        os.path.join(OUT_DIR, "{identity}_{length}", "panRG_make_prg_v0.5.0.k{kmer_size}.w{window_size}.panidx.zip")
    threads: 16
    resources:
        mem_mb=lambda wildcards, attempt: 30000 * attempt, threads=16
    params:
        pandora="software/pandora-linux-precompiled-v0.12.0-alpha.0",
        kmer_size="{kmer_size}",
        window_size="{window_size}",
    shell:
        "{params.pandora} index -w {params.window_size} -k {params.kmer_size} -t {threads} -o {output} {input.prg}"

rule make_panaroo_fasta:
    input:
        rules.rename_alignments.output,
        panaroo_dir=os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_output")
    output:
        output_fasta=directory(os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_annotations")),
        output_lengths=os.path.join(OUT_DIR, "{identity}_{length}", "gene_lengths.json")
    resources:
	    mem_mb=lambda wildcards, attempt: 10000 * attempt, threads=1
    threads: 1
    run:

        def reverse_complement(dna):
            complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}
            comp = ''.join([complement[base] if base in complement else base for base in dna[::-1]])
            return comp

        def get_first_allele(fasta_file):
            with open(fasta_file, 'r') as f:
                alignment = f.read().split(">")[1:]
            # Iterate through each line in the FASTA file
            for line in alignment:
                allele_header = line.split("\n")[0]
                sequence = "".join(line.split("\n")[1:]).replace("-", "").upper()
                break
            return os.path.basename(fasta_file).replace(".fasta", "").replace(".aln.fas", ""), allele_header, sequence

        def get_alleles(fasta_file):
            with open(fasta_file, 'r') as f:
                alignment = f.read().split(">")[1:]
            # Iterate through each line in the FASTA file
            annotations = {}
            for line in alignment:
                allele_header = line.split("\n")[0]
                sequence = "".join(line.split("\n")[1:]).replace("-", "").upper()
                annotations[allele_header] = sequence
            return os.path.basename(fasta_file).replace(".fasta", "").replace(".aln.fas", ""), annotations

        # list the MSAs
        msas = glob.glob(os.path.join(input[1], "aligned_gene_sequences", "*.fasta")) + glob.glob(os.path.join(input[1], "aligned_gene_sequences", "*.aln.fas"))
        random.shuffle(msas)
        # initialise the fake gff
        fasta_content = []
        current_base = 1
        gene_lengths = {}
        # make the output directory
        if not os.path.exists(output[0]):
            os.mkdir(output[0])
        # iterate through the MSAs
        sample_data = {}
        for f in tqdm(range(len(msas))):
            # get the header and sequence of the first MSA
            header, annotations = get_alleles(msas[f])
            # iterate through the annotations
            for a in annotations:
                sample = a.split(";")[0].replace("_R_", "")
                if sample not in sample_data:
                    sample_data[sample] = []
                sample_data[sample].append(f">{header}\n{annotations[a]}")
                # store the gene lengths
                if header not in gene_lengths or len(annotations[a]) < gene_lengths[header]:
                    gene_lengths[header] = len(annotations[a])
        for s in sample_data:
            with open(os.path.join(output[0], f"{s}.fasta"), "w") as o:
                o.write("\n".join(sample_data[s]))
        # write an output file for the gene lengths
        with open(output[1], "w") as o:
            o.write(json.dumps(gene_lengths))

rule make_truth_gff:
    input:
        assembly_dir="../pandora_assemblies",
        reads_dir="../pandora_data/samples",
        reference_fasta=os.path.join(OUT_DIR, "{identity}_{length}", "panaroo_annotations")
    output:
        directory(os.path.join(OUT_DIR, "{identity}_{length}", "truth_gffs"))
    threads: 24
    resources:
        mem_mb=30000
    run:
        for sample_path in glob.glob(os.path.join(input.reads_dir, "*")):
            sample_name = os.path.basename(sample_path)
            fastq = os.path.join(input.reads_dir, sample_name, sample_name + ".nanopore.fastq.gz")
            assembly = os.path.join(input.assembly_dir, sample_name + ".fa")
            out_gff = os.path.join(output[0], sample_name + ".gff")
            reference_fasta = os.path.join(input.reference_fasta, f"{sample_name}.fasta")
            command = f"python3 software/make_truth_with_minimap.py {assembly} {reference_fasta} {out_gff}"
            if not os.path.exists(output[0]):
                os.mkdir(output[0])
            shell(command)

rule get_true_read_annotations:
    input:
        reads_dir="../pandora_data/samples",
        gff_dir=os.path.join(OUT_DIR, "{identity}_{length}", "truth_gffs")
    output:
        json_file=os.path.join(OUT_DIR, "{identity}_{length}", "read_truth_annotations.json"),
        concatenated_reads=os.path.join(OUT_DIR, "{identity}_{length}", "all_reads.fastq.gz")
    threads: 8
    resources:
	    mem_mb=20000, threads=16
    run:
        def join_fastqs(fastq_files, output_file):
            import subprocess
            # Construct the Bash command
            if output_file.endswith('.gz'):
                # Use gzip for on-the-fly compression
                command = ["cat"] + fastq_files + ["|", "gzip", ">", output_file]
                bash_command = f"cat {' '.join(fastq_files)} | gzip > {output_file}"
            else:
                # Simple concatenation
                bash_command = f"cat {' '.join(fastq_files)} > {output_file}"

            try:
                # Execute the Bash command
                subprocess.run(bash_command, shell=True, check=True, executable="/bin/bash")
                print(f"Successfully joined {len(fastq_files)} FASTQ files into {output_file}")
            except subprocess.CalledProcessError as e:
                print(f"Error during joining FASTQ files: {e}")
                raise
        import shutil
        if os.path.exists(os.path.join(os.path.dirname(output[0]), "processing")):
            shutil.rmtree(os.path.join(os.path.dirname(output[0]), "processing"))
        if not os.path.exists(os.path.join(os.path.dirname(output[0]), "processing")):
            os.mkdir(os.path.join(os.path.dirname(output[0]), "processing"))
        all_read_annotations = {}
        fastq_files = []
        for sample_path in tqdm(glob.glob(os.path.join(input.reads_dir, "*"))):
            sample_name = os.path.basename(sample_path)
            fastq = os.path.join(input.reads_dir, sample_name, sample_name + ".nanopore.fastq.gz")
            output_json = os.path.join(os.path.dirname(output[0]), "processing", sample_name + ".json")
            bam_file = os.path.join(os.path.dirname(output[0]), "processing", sample_name + ".bam")
            truth_gff = os.path.join(input.gff_dir, sample_name + ".gff")
            command = f"python3 software/make_truth_read_annotations.py --bam-file {bam_file} --fastq-file {fastq} --gff-file {truth_gff} --output-json {output_json} --threads {resources.threads}"
            shell(command)
            with open(output_json) as i:
                content = json.load(i)
            all_read_annotations.update(content)
            fastq_files.append(fastq)
        with open(output.json_file, "w") as o:
            o.write(json.dumps(all_read_annotations))
        if not os.path.exists(output.concatenated_reads):
            join_fastqs(fastq_files, output.concatenated_reads)

rule run_pandora:
    input:
        fastq_file="../pandora_data/samples/{sample}",
        panRG=os.path.join(OUT_DIR, "{identity}_{length}", "panRG_make_prg_v0.5.0.k{kmer_size}.w{window_size}.panidx.zip")
    output:
        directory(os.path.join(OUT_DIR, "{identity}_{length}", "pandora_output.k{kmer_size}.w{window_size}", "{sample}"))
    threads: 16
    resources:
        mem_mb=lambda wildcards, attempt: 30000 * attempt, threads=16
    params:
        pandora="software/pandora-linux-precompiled-v0.12.0-alpha.0",
    shell:
        "mkdir -p {output} && {params.pandora} map -t {threads} \
            --min-gene-coverage-proportion 0.5 --max-covg 10000 \
            -o {output} {input.panRG} {input.fastq_file}/*.nanopore.fastq.gz"

rule get_pandora_read_annotations:
    input:
        pandora_dir=os.path.join(OUT_DIR, "{identity}_{length}", "pandora_output.k{kmer_size}.w{window_size}", "{sample}")
    output:
        json_file=os.path.join(OUT_DIR, "{identity}_{length}", "{sample}", "pandora_annotations.k{kmer_size}.w{window_size}.json")
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: 10000 * attempt
    shell:
        "python3 software/convert_pandora_annotations.py --pandora-dir {input.pandora_dir} --output-json {output.json_file}"

rule join_pandora_read_annotations:
    input:
        lambda wildcards: expand(
            os.path.join(OUT_DIR, "{identity}_{length}", "{sample}", "pandora_annotations.k{kmer_size}.w{window_size}.json"),
            identity=wildcards.identity,
            length=wildcards.length,
            kmer_size=wildcards.kmer_size,
            window_size=wildcards.window_size,
            sample=SAMPLES  # SAMPLES is defined earlier in the Snakefile
        )
    output:
        os.path.join(OUT_DIR, "{identity}_{length}", "pandora_annotations_merged.k{kmer_size}.w{window_size}.json")
    run:
        import json
        all_annotations = {}
        for jf in input:
            with open(jf) as infile:
                all_annotations.update(json.load(infile))
        with open(output[0], "w") as outfile:
            json.dump(all_annotations, outfile)

rule compare_truth_and_pandora_perfect_read_annotations:
    input:
        truth_annotations=rules.get_true_read_annotations.output.json_file,
        pandora_annotations=rules.join_pandora_read_annotations.output,
        gene_lengths=rules.make_panaroo_fasta.output.output_lengths
    output:
        os.path.join(OUT_DIR, "{identity}_{length}", "summary_stats.k{kmer_size}.w{window_size}.txt")
    threads: 1
    resources:
	    mem_mb=lambda wildcards, attempt: 10000 * attempt, threads=1
    shell:
        "python3 software/compare_JSON_annotations.py --truth-annotation {input.truth_annotations} --pandora-annotation {input.pandora_annotations} --gene-lengths {input.gene_lengths} --output-stats {output}"

rule plot_results:
    input:
        get_valid_outputs(
            expand(
                os.path.join(OUT_DIR, "{identity}_{length}", "summary_stats.k{kmer_size}.w{window_size}.txt"),
                identity=identities,
                length=lengths,
                kmer_size=kmer_sizes,
                window_size=window_sizes
                )
            )
    output:
        "result_plots_k_vs_w.eval.pdf",
        "result_plots_identity_vs_length.eval.pdf"
    threads: 1
    resources:
	    mem_mb=lambda wildcards, attempt: 10000 * attempt, threads=1
    shell:
        "python3 software/plot_recall_and_precision_eval.py"

rule compare_truth_and_pandora_individual:
    input:
        truth_annotations=rules.get_true_read_annotations.output.json_file,
        pandora_annotations=os.path.join(OUT_DIR, "{identity}_{length}", "{sample}", "pandora_annotations.k15.w5.json"),
        gene_lengths=rules.make_panaroo_fasta.output.output_lengths
    output:
        os.path.join(OUT_DIR, "{identity}_{length}", "summary_stats.{sample}.k15.w5.txt")
    threads: 1
    resources:
	    mem_mb=lambda wildcards, attempt: 10000 * attempt, threads=1
    run:
        if not os.path.exists(os.path.dirname(output[0])):
            os.mkdir(os.path.dirname(output[0]))
        shell(
            f"python3 software/compare_JSON_annotations_split.py --chromosome-reads pandora_assessment_with_AMR_filtered/0.8_0/reads_longest_contig.txt --plasmid-reads pandora_assessment_with_AMR_filtered/0.8_0/reads_other_contigs.txt --truth-annotation {input.truth_annotations} --pandora-annotation {input.pandora_annotations} --gene-lengths {input.gene_lengths} --output-stats {output}"
        )